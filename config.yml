dataset: idea # you can configure datasets with configs/datasets.yml

test_size: 0.1

tmpdir: null # if null default tmp directory will be used
log_file: data/logs/log.txt # set null for stdout
models_directory: text_models/saved
docs_directory: data/docs
docs_formats: ['html', 'pdf', 'md', 'rst'] # supported format extensions
model_types: ['TASK', 'PT+TASK', 'DOC+TASK', 'PT+DOC+TASK'] # supported training/evaluation types

models_suffixes:
  from_scratch: _task.model
  pretrained: _pt_task.model
  finetuned: _pt_doc_task.model
  doc_task: _doc_task.model


evaluation:
  text_model: siamese # see available text models in the corresponding section
  approach: simple # see available approaches in the corresponding section
  topns: [1, 5, 10, 15, 20, 25]
  save_results: true
  save_graph: true
  results_path: ./results

  is_tasks_test: true # available only if siamese text model will be used


approaches:
  intersection:
    min_count: 1

  tf_idf:
    weight: 0.7 # weight of tf-idf vectors similarity score in final score


models:
  random:
    vector_size: 300
    min_count: 1
    rand_by_w2v: false
    random_seed: 42
    save_to_path: text_models/saved

  word2vec:
    epochs: 100
    vector_size: 300
    min_count: 1
    tmp_file: null
    pretrained_model: word2vec-google-news-300 # must match vector_size
    seed: 42
    save_to_path: text_models/saved

  fasttext:
    epochs: 100
    vector_size: 300
    min_count: 1
    pretrained_model: text_models/pretrained/cc.en.300.bin # must match vector_size
    seed: 42
    save_to_path: text_models/saved

  bert:
    vector_size: 300
    epochs: 2
    batch_size: 16
    mask_probability: 0.15
    max_len: 512
    tmp_file: null
    pretrained_model: sentence-transformers/all-MiniLM-L6-v2
    device: cpu # 'cuda' or 'cpu'
    seed: 42
    save_to_path: text_models/saved

  sbert:
    vector_size: 300
    epochs: 2
    batch_size: 8
    warmup_steps: 0.1 # share of train data
    max_len: 512
    tmp_file: null
    n_examples: 5000 # number of bug reports pairs for train
    forget_const: 10
    pretrained_model: all-MiniLM-L6-v2
    seed: 42
    save_to_path: text_models/saved
    
  siamese:
    vector_size: 300
    epochs: 10
    batch_size: 8
    n_examples: all
    save_best_model: true # When set to True, save_steps must be a round multiple of eval_steps.
    warmup_rate: 0.1 # percent of training data
    max_len: 512
    task_loss: triplet  # 'triplet' or 'cossim'
    finetuning_strategies: ['mlm', 'tsdae', 'sase'] # see available strategies in 'bert_tasks' section
    pretrained_model: bert-base-uncased
    evaluation_steps: 500
    val_size: 0.1
    tmp_file: null
    device: cuda # 'cpu' or 'cuda'
    start_train_from_task: false
    seed: 42
    save_to_path: text_models/saved

    evaluator_config:
      batch_size: 8
      precision_recall_at_k: [1, 5, 10, 15, 20]
      accuracy_at_k: [1, 5, 10, 15, 20] # success rate like metric
      map_at_k: [5, 10] # validation metric


bert_tasks:
  mlm:
    epochs: 10
    batch_size: 8
    eval_steps: 250
    n_examples: all
    mask_probability: 0.15
    save_steps: 250
    save_best_model: true # When set to True, save_steps must be a round multiple of eval_steps.

  nsp:
    epochs: 2
    batch_size: 8
    eval_steps: 250
    n_examples: all
    forget_const: 10
    save_steps: 250
    save_best_model: true # When set to True, save_steps must be a round multiple of eval_steps.

  sase:
    epochs: 10
    batch_size: 8
    eval_steps: 250
    n_examples: all
    save_steps: 250
    save_best_model: true # When set to True, save_steps must be a round multiple of eval_steps.

  sts:
    epochs: 2
    batch_size: 8
    eval_steps: 250
    n_examples: all
    forget_const: 10
    save_best_model: true # When set to True, save_steps must be a round multiple of eval_steps.

  tsdae:
    epochs: 10
    batch_size: 8
    eval_steps: 250
    n_examples: all
    save_best_model: true # When set to True, save_steps must be a round multiple of eval_steps.
