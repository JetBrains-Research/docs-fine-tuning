dataset: idea # you can configure datasets with configs/datasets.yml

test_size: 0.1

tmpdir: null # if null default tmp directory will be used
log_file: null # set null for stdout
docs_directory: data/docs
docs_formats: ['html', 'pdf', 'md', 'rst'] # supported format extensions
model_types: ['PT_TASK'] # supported training/evaluation types

text_model: bert # see available text models in the corresponding section
target_task: assignment_recommendation # see available tasks in 'target_tasks' section


evaluation:
  approach: assignment # see available approaches in the corresponding section
  topns: [1, 5, 10, 15, 20, 25]
  save_results: true
  save_graph: true
  models_path: text_models/saved/test
  results_path: ./results/last/idea/bert_so

  is_tasks_test: true # available only if siamese text model will be used


approaches:
  intersection:
    min_count: 1

  tf_idf:
    weight: 0.7 # weight of tf-idf vectors similarity score in final score


models:
  random:
    vector_size: 300
    min_count: 1
    rand_by_w2v: false
    seed: 42
    save_to_path: text_models/saved

  word2vec:
    epochs: 100
    vector_size: 300
    min_count: 1
    tmp_file: null
    pretrained_model: word2vec-google-news-300 # must match vector_size
    seed: 42
    save_to_path: text_models/saved

  fasttext:
    epochs: 100
    vector_size: 300
    min_count: 1
    pretrained_model: text_models/pretrained/cc.en.300.bin # must match vector_size
    seed: 42
    save_to_path: text_models/saved
    
  bert:
    domain_adaptation_tasks: ['mlm', 'tsdae'] # see available strategies in 'bert_tasks' section
    pretrained_model: bert-base-uncased # giganticode/bert-base-StackOverflow-comments_1M
    start_train_from: null # bugs / task / null
    seed: 42
    save_to_path: text_models/saved/test
    report_wandb: false

    wandb_config:
      project: null # insert your project
      entity: null # insert entity


target_tasks:
  duplicates_detection:
    vector_size: 300
    epochs: 3
    batch_size: 16
    n_examples: all
    save_best_model: true # When set to True, save_steps must be a round multiple of eval_steps.
    warmup_ratio: 0.1 # percent of training data
    weight_decay: 0.001
    learning_rate: 1e-5
    max_len: 256
    loss: triplet # 'triplet' or 'cossim'
    pooling_mode: mean # mean/max/cls
    evaluation_steps: null
    save_steps: null
    val_size: 0.1
    seed: 42
    device: cuda # 'cpu' or 'cuda'

    evaluator_config:
      batch_size: 16
      precision_recall_at_k: [ 5 ]
      accuracy_at_k: [ 1, 5, 10, 15, 20 ] # success rate like metric
      map_at_k: [ 5, 10 ] # validation metric

  assignment_recommendation:
    epochs: 3
    batch_size: 16
    n_examples: all
    save_best_model: true # When set to True, save_steps must be a round multiple of eval_steps.
    warmup_ratio: 0.1 # percent of training data
    weight_decay: 0.001
    learning_rate: 1e-5
    dropout_ratio: 0.5
    max_len: 256
    pooling_mode: mean # mean/max/cls
    evaluation_steps: null
    save_steps: null
    val_size: 0.1
    seed: 42
    device: cuda # 'cpu' or 'cuda'
    evaluator_config:
      batch_size: 16


dapt_tasks:
  mlm:
    epochs: 23
    batch_size: 64
    eval_steps: null # if null then epoch mode will be used
    save_steps: null # if null then epoch mode will be used
    n_examples: all
    mask_probability: 0.15
    val: 0.1
    metric_for_best_model: loss # 'task_map' or 'loss' or 'loss_task'
    save_best_model: true # When set to True, save_steps must be a round multiple of eval_steps.
    do_eval_on_artefacts: true
    max_len: 128 # if null then default max supported seq length by model will be used
    warmup_ratio: 0.05
    weight_decay: 0.01

  nsp:
    epochs: 2
    batch_size: 8
    eval_steps: null # if null then epoch mode will be used
    save_steps: null # if null then epoch mode will be used
    n_examples: all
    forget_const: 10
    val: 0.1
    metric_for_best_model: task_map # 'task_map' or 'loss' or 'loss_task'
    save_best_model: true # When set to True, save_steps must be a round multiple of eval_steps.
    do_eval_on_artefacts: true
    max_len: 128 # if null then default max supported seq length by model will be used
    warmup_ratio: 0.0
    weight_decay: 0.0

  sase:
    epochs: 8
    batch_size: 8
    eval_steps: null # if null then epoch mode will be used
    save_steps: null # if null then epoch mode will be used
    n_examples: all
    val: 0.1
    metric_for_best_model: task_map # 'task_map' or 'loss' or 'loss_task'
    save_best_model: true # When set to True, save_steps must be a round multiple of eval_steps.
    do_eval_on_artefacts: true
    max_len: 128 # if null then default max supported seq length by model will be used
    warmup_ratio: 0.0
    weight_decay: 0.0

  sts:
    epochs: 2
    batch_size: 8
    eval_steps: null # if null then epoch mode will be used
    save_steps: null # if null then epoch mode will be used
    n_examples: all
    forget_const: 10
    val: 0.1
    metric_for_best_model: task_map # 'task_map' or 'loss' or 'loss_task'
    save_best_model: true # When set to True, save_steps must be a round multiple of eval_steps.
    do_eval_on_artefacts: true
    max_len: 128 # if null then default max supported seq length by model will be used
    warmup_ratio: 0.0
    weight_decay: 0.0
    pooling_mode: mean # mean/max/cls

  tsdae:
    epochs: 23
    batch_size: 64
    eval_steps: null # if null then epoch mode will be used
    save_steps: null # if null then epoch mode will be used
    n_examples: all
    val: 0.1
    metric_for_best_model: loss # 'task_map' or 'loss' or 'loss_task'
    save_best_model: true # When set to True, save_steps must be a round multiple of eval_steps.
    do_eval_on_artefacts: true
    max_len: 128 # if null then default max supported seq length by model will be used
    warmup_ratio: 0.0
    weight_decay: 0.0
    pooling_mode: mean # mean/max/cls
