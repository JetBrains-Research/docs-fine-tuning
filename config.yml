datasets:
  full: data/csv/KT2020.csv
  train: data/csv/train_KT_2.csv
  test: data/csv/test_KT_2.csv

test_size: 0.1

models_directory: text_models/saved
docs_directory: data/docs
docs_formats: ['html', 'pdf', 'md'] # supported formats

models_suffixes:
  from_scratch: .model
  pretrained: _pretrained.model
  finetuned: _finetuned.model


evaluation:
  text_model: siamese # see available text models in the corresponding section
  approach: simple # see available approaches in the corresponding section
  topns: [1, 5, 10, 15, 20, 25]
  save_results: true
  save_graph: true
  results_path: ./results/kotlin

  is_tasks_test: true # available only if siamese text model will be used


approaches:
  intersection:
    min_count: 1

  tf_idf:
    weight: 0.7 # weight of tf-idf vectors similarity score in final score


models:
  random:
    vector_size: 300
    min_count: 1
    rand_by_w2v: false
    random_seed: 42
    save_to_path: text_models/saved

  word2vec:
    epochs: 100
    vector_size: 300
    min_count: 1
    tmp_file: null
    pretrained_model: word2vec-google-news-300 # must match vector_size
    seed: 42
    save_to_path: text_models/saved

  fasttext:
    epochs: 100
    vector_size: 300
    min_count: 1
    pretrained_model: text_models/pretrained/cc.en.300.bin # must match vector_size
    seed: 42
    save_to_path: text_models/saved

  bert:
    vector_size: 300
    epochs: 2
    batch_size: 16
    mask_probability: 0.15
    max_len: 512
    tmp_file: null
    pretrained_model: sentence-transformers/all-MiniLM-L6-v2
    device: cpu # 'cuda' or 'cpu'
    seed: 42
    save_to_path: text_models/saved

  sbert:
    vector_size: 300
    epochs: 2
    batch_size: 16
    warmup_steps: 0.1 # share of train data
    max_len: 512
    tmp_file: null
    n_examples: 5000 # number of bug reports pairs for train
    forget_const: 10
    pretrained_model: all-MiniLM-L6-v2
    seed: 42
    save_to_path:
    
  siamese:
    vector_size: 300
    epochs: 2
    batch_size: 16
    n_examples: all
    warmup_rate: 0.1 # percent of training data
    max_len: 512
    task_loss: triplet  # 'triplet' or 'cossim'
    finetuning_strategies: ['mlm', 'nsp', 'tsdae'] # see available strategies in 'bert_tasks' section
    pretrained_model: bert-base-uncased
    evaluation_steps: 500
    val_size: 0.1
    tmp_file: null
    device: cpu # 'cpu' or 'cuda'
    seed: 42
    save_to_path: text_models/saved


bert_tasks:
  mlm:
    epochs: 2
    batch_size: 16
    eval_steps: 100
    n_examples: all
    mask_probability: 0.15
    save_steps: 100

  nsp:
    epochs: 2
    batch_size: 16
    eval_steps: 200
    n_examples: all
    forget_const: 10
    save_steps: 100

  sts:
    epochs: 2
    batch_size: 16
    eval_steps: 200
    n_examples: all
    forget_const: 10

  tsdae:
    epochs: 2
    batch_size: 16
    eval_steps: 200
    n_examples: all